{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TAwTeAS3UEc"
      },
      "source": [
        "## Assignment 2\n",
        "Transformer 모델의 Scaled dot-product 어텐션 메커니즘을 구현해보세요. 그리고 ViT(Vision Transformer)에서 이 부분이 어떤 의미로 동작하는지 임의의 숫자를 넣어 만든 Tensor 데이터를 활용하여 간단한 코드를 작성하여 설명하세요.\n",
        "\n",
        "- Requirements\n",
        "1. C++ 또는 Python 이용하여 작성하세요.\n",
        "2. TensorFlow, Pytorch, JAX 등 딥러닝 프레임워크 사용 가능 (Keras와 같은 상대적으로 고수준의 API는 사용 불가)\n",
        "3. 그 외에 라이브러리 사용 불가능"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "설명) Scaled Dot-Product Attention 메커니즘은 입력 시퀀스의 각 원소가 출력 시퀀스에 어떻게 영향을 미치는지를 결정하는데 중요한 역할을 한다. 이 메커니즘은 Query, key, Value 벡터를 사용하여 각 입력에 대한 가중치를 계산하고, 이를 통해 출력을 생성한다.\n",
        "\n",
        "ViT에서는 이 Attention 메커니즘을 이미지의 패치에 적용하여, 이미지 내의 어떤 부분이 중요한지 결정한다. 각 패치는 Query, Key, Value로 변환되고, 이를 통해 모델은 이미지 내의 관련성 높은 부분에 집중할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VMsGsg_3JF8",
        "outputId": "c295f10c-45f9-4706-efa8-3f5bd263951e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output Tensor : tensor([[[0.4946, 0.5674, 0.4929,  ..., 0.3946, 0.4969, 0.2857],\n",
            "         [0.4945, 0.5820, 0.4963,  ..., 0.3817, 0.4925, 0.2895],\n",
            "         [0.4960, 0.5380, 0.4866,  ..., 0.4204, 0.5072, 0.2776]]])\n",
            "Attention Map : tensor([[[0.3582, 0.3149, 0.3269],\n",
            "         [0.3593, 0.2994, 0.3413],\n",
            "         [0.3580, 0.3456, 0.2964]]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def scaled_dot_product_attention(query, key, value):\n",
        "  # query, key, value는 모두 (batch_size, seq_len, d_model)의 shape를 가진다.\n",
        "    d_k = query.size(-1)  # d_model의 크기\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "    p_attn = F.softmax(scores, dim=-1)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "\n",
        "# 임의의 tensor 데이터 생성\n",
        "batch_size = 1\n",
        "seq_len = 3\n",
        "d_model = 512\n",
        "\n",
        "# Query, key, value를 임의의 값으로 초기화\n",
        "query = torch.rand(batch_size, seq_len, d_model)\n",
        "key = torch.rand(batch_size, seq_len, d_model)\n",
        "value = torch.rand(batch_size, seq_len, d_model)\n",
        "\n",
        "# print(query)\n",
        "\"\"\"\n",
        "tensor([[[0.3191, 0.7686, 0.2709,  ..., 0.5455, 0.2859, 0.3686],\n",
        "         [0.7467, 0.3468, 0.3531,  ..., 0.2891, 0.9572, 0.4493],\n",
        "         [0.5958, 0.2423, 0.6304,  ..., 0.9418, 0.4219, 0.7905]]])\n",
        "\"\"\"\n",
        "# print(query.size(-1))\n",
        "\"\"\"\n",
        "512\n",
        "\"\"\"\n",
        "# Attention 메커니즘 실행\n",
        "output, attention = scaled_dot_product_attention(query, key, value)\n",
        "\n",
        "print(f\"Output Tensor : {output}\")\n",
        "print(f\"Attention Map : {attention}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "scaled_dot_product_attention함수는 Query, Key, Value 텐서를 받아 Attention Map을 계산하고, 이를 사용하여 출력 텐서를 생성한다. 이 과정에서 Query와 Key의 내적(dot product)을 계산하고, 이를 d_model의 크기의 제곱근으로 나누어 스케일링한다. 그 후 softmax함수를 적용하여 각 key에 대한 가중치(Attention Map)을 계산하고, 이를 Value에 적용하여 최종 출력을 얻는다.\n",
        "\n",
        "ViT에서는 이러한 Attention 메커니즘을 이미지의 각 패치에 적용하여, 중요한 특징을 추출하고 이미지를 분류하는 데 사용한다. 각 패치는 Query, Key, Value로 변환되고, Attention메커니즘을 통해 모델은 이미지 내의 중요한 부분에 집중하게 된다. 이를 통해 모델은 전체 이미지를 보는 대신, 중요한 부분에만 집중하여 더 효율적으로 이미지를 이해하고 분류할수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4jKN2Kx5yiq",
        "outputId": "ab6b110f-c38d-434d-87d3-ef9070757812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output Tensor : tensor([[[0.4993, 0.5175, 0.5016,  ..., 0.4946, 0.5057, 0.4996],\n",
            "         [0.5006, 0.5197, 0.5013,  ..., 0.4954, 0.5070, 0.4986],\n",
            "         [0.5013, 0.5187, 0.5021,  ..., 0.4901, 0.5094, 0.4932],\n",
            "         ...,\n",
            "         [0.5040, 0.5188, 0.5048,  ..., 0.4955, 0.5100, 0.4977],\n",
            "         [0.5019, 0.5217, 0.5011,  ..., 0.4953, 0.5091, 0.4960],\n",
            "         [0.4995, 0.5192, 0.5030,  ..., 0.4953, 0.5104, 0.4965]],\n",
            "\n",
            "        [[0.5022, 0.4838, 0.4951,  ..., 0.5017, 0.5040, 0.4921],\n",
            "         [0.5035, 0.4877, 0.4954,  ..., 0.5025, 0.5023, 0.4976],\n",
            "         [0.5039, 0.4884, 0.4945,  ..., 0.5023, 0.5060, 0.4946],\n",
            "         ...,\n",
            "         [0.5021, 0.4853, 0.4936,  ..., 0.5055, 0.5073, 0.4970],\n",
            "         [0.5012, 0.4867, 0.4926,  ..., 0.5013, 0.5056, 0.4978],\n",
            "         [0.5006, 0.4857, 0.4900,  ..., 0.5041, 0.5060, 0.4954]],\n",
            "\n",
            "        [[0.5057, 0.5061, 0.4827,  ..., 0.5197, 0.5022, 0.4984],\n",
            "         [0.5049, 0.5042, 0.4845,  ..., 0.5185, 0.4996, 0.4978],\n",
            "         [0.5043, 0.5041, 0.4858,  ..., 0.5202, 0.5019, 0.4977],\n",
            "         ...,\n",
            "         [0.5060, 0.5041, 0.4862,  ..., 0.5200, 0.5009, 0.4982],\n",
            "         [0.5030, 0.5012, 0.4840,  ..., 0.5199, 0.4999, 0.5001],\n",
            "         [0.5051, 0.5038, 0.4852,  ..., 0.5170, 0.5033, 0.4959]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.4590, 0.4862, 0.4794,  ..., 0.5018, 0.5165, 0.5256],\n",
            "         [0.4558, 0.4861, 0.4790,  ..., 0.5010, 0.5142, 0.5290],\n",
            "         [0.4572, 0.4865, 0.4814,  ..., 0.5012, 0.5162, 0.5272],\n",
            "         ...,\n",
            "         [0.4552, 0.4871, 0.4796,  ..., 0.5039, 0.5130, 0.5274],\n",
            "         [0.4572, 0.4866, 0.4805,  ..., 0.4991, 0.5137, 0.5280],\n",
            "         [0.4576, 0.4865, 0.4810,  ..., 0.4997, 0.5144, 0.5276]],\n",
            "\n",
            "        [[0.4910, 0.4961, 0.4786,  ..., 0.5076, 0.5234, 0.4923],\n",
            "         [0.4927, 0.4973, 0.4768,  ..., 0.5125, 0.5231, 0.4939],\n",
            "         [0.4947, 0.4956, 0.4764,  ..., 0.5131, 0.5223, 0.4917],\n",
            "         ...,\n",
            "         [0.4943, 0.4948, 0.4796,  ..., 0.5146, 0.5220, 0.4941],\n",
            "         [0.4941, 0.4945, 0.4765,  ..., 0.5139, 0.5252, 0.4926],\n",
            "         [0.4933, 0.4959, 0.4787,  ..., 0.5133, 0.5222, 0.4956]],\n",
            "\n",
            "        [[0.4859, 0.5384, 0.4930,  ..., 0.4455, 0.4928, 0.4958],\n",
            "         [0.4869, 0.5384, 0.4934,  ..., 0.4451, 0.4949, 0.4983],\n",
            "         [0.4863, 0.5426, 0.4943,  ..., 0.4430, 0.4920, 0.4983],\n",
            "         ...,\n",
            "         [0.4871, 0.5410, 0.4955,  ..., 0.4442, 0.4876, 0.4954],\n",
            "         [0.4881, 0.5409, 0.4934,  ..., 0.4451, 0.4907, 0.4944],\n",
            "         [0.4870, 0.5425, 0.4913,  ..., 0.4437, 0.4924, 0.4960]]])\n",
            "Attention Map : tensor([[[0.0030, 0.0034, 0.0049,  ..., 0.0037, 0.0033, 0.0034],\n",
            "         [0.0039, 0.0034, 0.0044,  ..., 0.0043, 0.0034, 0.0041],\n",
            "         [0.0036, 0.0026, 0.0041,  ..., 0.0036, 0.0039, 0.0043],\n",
            "         ...,\n",
            "         [0.0032, 0.0033, 0.0045,  ..., 0.0039, 0.0034, 0.0033],\n",
            "         [0.0038, 0.0034, 0.0049,  ..., 0.0043, 0.0038, 0.0038],\n",
            "         [0.0039, 0.0036, 0.0047,  ..., 0.0044, 0.0036, 0.0041]],\n",
            "\n",
            "        [[0.0040, 0.0037, 0.0029,  ..., 0.0027, 0.0049, 0.0042],\n",
            "         [0.0037, 0.0033, 0.0025,  ..., 0.0028, 0.0050, 0.0041],\n",
            "         [0.0035, 0.0032, 0.0032,  ..., 0.0027, 0.0055, 0.0037],\n",
            "         ...,\n",
            "         [0.0034, 0.0038, 0.0031,  ..., 0.0030, 0.0066, 0.0046],\n",
            "         [0.0039, 0.0027, 0.0026,  ..., 0.0030, 0.0062, 0.0042],\n",
            "         [0.0035, 0.0035, 0.0029,  ..., 0.0034, 0.0056, 0.0035]],\n",
            "\n",
            "        [[0.0052, 0.0050, 0.0038,  ..., 0.0039, 0.0044, 0.0033],\n",
            "         [0.0051, 0.0051, 0.0037,  ..., 0.0041, 0.0047, 0.0031],\n",
            "         [0.0048, 0.0047, 0.0041,  ..., 0.0040, 0.0047, 0.0038],\n",
            "         ...,\n",
            "         [0.0048, 0.0054, 0.0036,  ..., 0.0043, 0.0044, 0.0030],\n",
            "         [0.0043, 0.0047, 0.0035,  ..., 0.0039, 0.0046, 0.0037],\n",
            "         [0.0053, 0.0047, 0.0034,  ..., 0.0044, 0.0049, 0.0037]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.0044, 0.0033, 0.0050,  ..., 0.0048, 0.0044, 0.0047],\n",
            "         [0.0045, 0.0032, 0.0054,  ..., 0.0031, 0.0037, 0.0040],\n",
            "         [0.0043, 0.0039, 0.0049,  ..., 0.0037, 0.0037, 0.0049],\n",
            "         ...,\n",
            "         [0.0046, 0.0028, 0.0049,  ..., 0.0030, 0.0039, 0.0048],\n",
            "         [0.0039, 0.0034, 0.0049,  ..., 0.0035, 0.0037, 0.0042],\n",
            "         [0.0047, 0.0033, 0.0049,  ..., 0.0038, 0.0040, 0.0047]],\n",
            "\n",
            "        [[0.0037, 0.0028, 0.0035,  ..., 0.0039, 0.0044, 0.0030],\n",
            "         [0.0037, 0.0028, 0.0041,  ..., 0.0041, 0.0042, 0.0031],\n",
            "         [0.0042, 0.0024, 0.0043,  ..., 0.0046, 0.0045, 0.0036],\n",
            "         ...,\n",
            "         [0.0037, 0.0030, 0.0035,  ..., 0.0040, 0.0043, 0.0028],\n",
            "         [0.0043, 0.0027, 0.0042,  ..., 0.0040, 0.0048, 0.0036],\n",
            "         [0.0044, 0.0029, 0.0036,  ..., 0.0039, 0.0051, 0.0034]],\n",
            "\n",
            "        [[0.0050, 0.0055, 0.0044,  ..., 0.0031, 0.0035, 0.0033],\n",
            "         [0.0051, 0.0055, 0.0038,  ..., 0.0035, 0.0029, 0.0032],\n",
            "         [0.0047, 0.0054, 0.0037,  ..., 0.0033, 0.0038, 0.0037],\n",
            "         ...,\n",
            "         [0.0047, 0.0058, 0.0037,  ..., 0.0030, 0.0037, 0.0038],\n",
            "         [0.0038, 0.0050, 0.0038,  ..., 0.0035, 0.0035, 0.0033],\n",
            "         [0.0055, 0.0057, 0.0035,  ..., 0.0034, 0.0035, 0.0035]]])\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "  # 이미지패치를 나타내는 tensor 데이터를 input 넣은 경우\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def scaled_dot_product_attention(query, key, value):\n",
        "  # query, key, value는 모두 (batch_size, seq_len, d_model)의 shape를 가진다.\n",
        "    d_k = query.size(-1)  # d_model의 크기\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "    p_attn = F.softmax(scores, dim=-1)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "\n",
        "# 임의의 이미지패치를 나타내는 tensor 데이터 생성\n",
        "patch_size = 16 # 16x16 pixel의 패치\n",
        "num_patches = 256 # 256개의 패치\n",
        "d_model = 512 # Transformer의 차원\n",
        "\n",
        "# Query, key, value를 임의의 값으로 초기화\n",
        "query = torch.rand(patch_size, num_patches, d_model)\n",
        "key = torch.rand(patch_size, num_patches, d_model)\n",
        "value = torch.rand(patch_size, num_patches, d_model)\n",
        "\n",
        "# Attention 메커니즘 실행\n",
        "output, attention = scaled_dot_product_attention(query, key, value)\n",
        "\n",
        "print(f\"Output Tensor : {output}\")\n",
        "print(f\"Attention Map : {attention}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "설명 ) 이미지의 각 패치를 Query, Key, Value로 변환하고, Attention 메커니즘을 통해 중요한 패치에 더 많은 가중치를 부여한다. 이렇게 계산된 출력은 이미지의 중요한 특징을 포함하고 있으며, 이를 통해 모델은 이미지를 더 잘 이해하고 분류할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVfD-sU-604M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ6HlQiR7DGK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
